{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a17c68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-09 20:27:41 - Your app is available at http://localhost:8000\n",
      "2025-07-09 20:27:43 - Translated markdown file for en-US not found. Defaulting to chainlit.md.\n",
      "are we appending this more than once?\n",
      "2025-07-09 20:29:01 - Skipping data after last boundary\n",
      "=== MESSAGE RECEIVED: What does this file look like to you? ===\n",
      "=== FILE ATTACHMENTS RECEIVED ===\n",
      "2025-07-09 20:29:09 - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== SENDING RESPONSE: This file, `causal_product_cuda.cu`, looks like a CUDA/C++ source file implementing high-performance custom CUDA kernels for **causal dot-product attention**, typically used in deep learning—particularly in transformer models where causal (autoregressive) attention is required (e.g., for language modeling). The main features and observations are:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **CUDA Kernels for Attention**\n",
      "\n",
      "- The file contains multiple CUDA kernel implementations (e.g., `causal_dot_product_kernel`, `causal_dot_backward_query_key_kernel`, etc.) designed to run on GPUs for efficient computation of forward and backward passes of causal attention.\n",
      "- Shared memory is used for performance optimization.\n",
      "- There’s heavy loop unrolling, atomic operations, and other standard CUDA optimizations.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **NVIDIA Optimizations**\n",
      "\n",
      "- There is a large code block within the `nvidia` namespace, enabled using `#define ENABLE_NVIDIA_OPTIMIZATIONS`.\n",
      "- These sections offer highly optimized paths for specific tensor layouts and CUDA hardware, leveraging warp and shared memory structures.\n",
      "- There are templated functions and device/host code (`__device__ __host__`).\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Integration with PyTorch**\n",
      "\n",
      "- At the end, there is a `PYBIND11_MODULE` block, exposing some functions (like `causal_dot_product` and `causal_dot_backward`) to Python as a PyTorch extension.\n",
      "- All tensor inputs and outputs use the PyTorch C++ API (`torch::Tensor`, `PackedTensorAccessor32`, etc.).\n",
      "- It uses the C++ extension API to integrate custom CUDA ops with PyTorch.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **Purpose and Use-Case**\n",
      "\n",
      "- The file is likely a core part of a custom attention mechanism for transformer-type models, focused on achieving high-speed (possibly low-memory) implementations of the causal dot-product operation and its gradient/backward pass.\n",
      "- Used in contexts where standard PyTorch attention is not fast enough, or when large-sequence models (like large language models) require specialized kernel optimizations.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. **Copyright and Authorship**\n",
      "\n",
      "- It credits researchers from the Idiap Research Institute and NVIDIA.\n",
      "- The copyright/license blocks indicate:\n",
      "  - Originated from academic research.\n",
      "  - Parts are contributed/modified by NVIDIA.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. **Fallback Mechanisms**\n",
      "\n",
      "- The code checks if the NVIDIA-optimized path is available; otherwise, it falls back to a native implementation.\n",
      "\n",
      "---\n",
      "\n",
      "**Summary:**  \n",
      "This file is a **custom CUDA implementation for causal dot-product attention** and its gradients, highly optimized (with special NVIDIA kernels), meant as a PyTorch extension for deep learning models, especially those needing fast autoregressive/causal attention for long sequences.\n",
      "\n",
      "If you’d like an explanation of the logic of any specific kernel, an architectural overview, or details on how to build/use this as a PyTorch extension, let me know! ===\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "import importlib\n",
    "import sys\n",
    "from dev_tools import run_chainlit_thread\n",
    "\n",
    "thread = run_chainlit_thread(\"basics_app.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5d892b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='Welcome to the Chainlit app! I can perform long division and read file attachments. Try sending me a message or attaching a file.', additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content='File name: causal_product_cuda.cu\\nFile content:\\n//\\n// Copyright (c) 2020 Idiap Research Institute, http://www.idiap.ch/\\n// Written by Angelos Katharopoulos <angelos.katharopoulos@idiap.ch>,\\n// Apoorv Vyas <avyas@idiap.ch>\\n//\\n\\n//\\n// For modifications made inside namespace nvidia (authored by jdemouth):\\n//\\n// Copyright (c) 2021 NVIDIA CORPORATION. All rights reserved.\\n// \\n// Permission is hereby granted, free of charge, to any person obtaining a copy of\\n// this software and associated documentation files (the \"Software\"), to deal in\\n// the Software without restriction, including without limitation the rights to\\n// use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\\n// the Software, and to permit persons to whom the Software is furnished to do so,\\n// subject to the following conditions:\\n// \\n// The above copyright notice and this permission notice shall be included in all\\n// copies or substantial portions of the Software.\\n// \\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\\n// FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\\n// COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\\n// IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\\n// CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n//\\n\\n#include <torch/extension.h>\\n#include <assert.h>\\n#include <stdio.h>\\n\\n#define ENABLE_NVIDIA_OPTIMIZATIONS\\n\\n#ifdef ENABLE_NVIDIA_OPTIMIZATIONS\\nnamespace nvidia {\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\nconstexpr int THREADS_PER_WARP = 32;\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\nconstexpr int LOW_OCCUPANCY_THRESHOLD = 40; // TODO: Make it HW specific (like 1/2 SMs).\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\nstatic inline __device__ __host__ int div_up(int m, int n) {\\n  return (m + n-1) / n;\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\nstatic inline __device__ __host__ int round_up(int m, int n) {\\n  return div_up(m, n) * n;\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\ntemplate< typename T >\\nstruct Lmha_params {\\n\\n  // The output buffer. Dimensions [B, H, L, M].\\n  T *out;\\n\\n  // The input Qs. Dimensions [B, H, L, E].\\n  const T *q;\\n  // The input Ks. Dimensions [B, H, L, E].\\n  const T *k;\\n  // The input Vs. Dimensions [B, H, L, M].\\n  const T *v;\\n\\n  // The different dimensions.\\n  int B, L, H, E, M;\\n\\n  // The strides for the different tensors.\\n  int q_stride_B, q_stride_H, q_stride_L;\\n  int k_stride_B, k_stride_H, k_stride_L;\\n  int v_stride_B, v_stride_H, v_stride_L;\\n  int o_stride_B, o_stride_H, o_stride_L;\\n};\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\ntemplate< int E, bool GO_BACKWARD, int WARPS, int COLS_PER_THREAD = 4 >\\n__global__ __launch_bounds__(WARPS * THREADS_PER_WARP)\\nvoid lmha_low_occupancy_kernel(Lmha_params<float> params) {\\n\\n  // The number of threads per block.\\n  constexpr int THREADS_PER_BLOCK = WARPS * THREADS_PER_WARP;\\n  // The number of rows per thread.\\n  constexpr int ROWS_PER_THREAD = E / THREADS_PER_WARP;\\n  // The number of steps per iteration.\\n  constexpr int COLS_PER_ITER = WARPS * COLS_PER_THREAD;\\n\\n  // Make sure E is a multiple of the warp size.\\n  static_assert(E % THREADS_PER_WARP == 0, \"\");\\n\\n  // Shared memory to store V/O.\\n  __shared__ float smem_v[COLS_PER_ITER], smem_o[COLS_PER_ITER];\\n  // Shared memory buffer to performance the reductions.\\n  __shared__ float smem_reds[E * WARPS]; \\n\\n  // The sequence processed by that block.\\n  const int bi = blockIdx.z;\\n  // The head processed by that block.\\n  const int hi = blockIdx.y;\\n  // The hidden cell in the V/output buffers.\\n  const int vi = blockIdx.x;\\n\\n  // The linear index of the thread.\\n  const int tidx = threadIdx.x;\\n\\n  // Decompose the block in warp/lane.\\n  const int warp = tidx / THREADS_PER_WARP;\\n  const int lane = tidx % THREADS_PER_WARP;\\n\\n  // The base offset loaded by the thread in Q and K.\\n  int offset_q = bi*params.q_stride_B + hi*params.q_stride_H + lane;\\n  int offset_k = bi*params.k_stride_B + hi*params.k_stride_H + lane;\\n\\n  // If we walk backward, account for the extra offset.\\n  if( GO_BACKWARD ) {\\n    offset_q += (params.L-1)*params.q_stride_L;\\n    offset_k += (params.L-1)*params.k_stride_L;\\n  }\\n\\n  // Position the warp at the beginning of the proper timestep.\\n  if( GO_BACKWARD ) {\\n    offset_q -= warp*COLS_PER_THREAD*params.q_stride_L;\\n    offset_k -= warp*COLS_PER_THREAD*params.k_stride_L;\\n  } else {\\n    offset_q += warp*COLS_PER_THREAD*params.q_stride_L;\\n    offset_k += warp*COLS_PER_THREAD*params.k_stride_L;\\n  }\\n\\n  // Determine the base pointers for Q and K.\\n  const float *ptr_q = &params.q[offset_q];\\n  const float *ptr_k = &params.k[offset_k];\\n\\n  // Is a given row valid?\\n  int valid_qk[ROWS_PER_THREAD];\\n  #pragma unroll\\n  for( int ii = 0; ii < ROWS_PER_THREAD; ++ii ) {\\n    valid_qk[ii] = lane + ii*THREADS_PER_WARP < params.E;\\n  }\\n\\n  // The offset to the position loaded by the thread in V.\\n  int offset_v = bi*params.v_stride_B + hi*params.v_stride_H + vi;\\n  int offset_o = bi*params.o_stride_B + hi*params.o_stride_H + vi;\\n\\n  // If we walk backward, account for the extra offset.\\n  if( GO_BACKWARD ) {\\n    offset_v += (params.L-1)*params.v_stride_L;\\n    offset_o += (params.L-1)*params.o_stride_L;\\n  }\\n\\n  // We load/store a strided matrix of COLS_PER_ITER x OUTPUTS_PER_BLOCK.\\n  if( GO_BACKWARD ) {\\n    offset_v -= tidx*params.v_stride_L;\\n    offset_o -= tidx*params.o_stride_L;\\n  } else {\\n    offset_v += tidx*params.v_stride_L;\\n    offset_o += tidx*params.o_stride_L;\\n  }\\n\\n  // Determine the base pointer for V.\\n  const float *ptr_v = &params.v[offset_v];\\n  // The output pointer. \\n  float *ptr_o = &params.out[offset_o];\\n\\n  // The running KVs.\\n  float running_kv[ROWS_PER_THREAD];\\n  #pragma unroll\\n  for( int ri = 0; ri < ROWS_PER_THREAD; ++ri ) {\\n    running_kv[ri] = 0.f;\\n  }\\n\\n  // Iterate over the timesteps. TODO: Use params.loop_count!!!\\n  for( int iter = 0; iter < params.L; iter += COLS_PER_ITER ) {\\n\\n    // Each thread loads a matrix of elements.\\n    float q[ROWS_PER_THREAD][COLS_PER_THREAD], k[ROWS_PER_THREAD][COLS_PER_THREAD];\\n\\n    // Trigger the memory loads for Q and K.\\n    #pragma unroll\\n    for( int ci = 0; ci < COLS_PER_THREAD; ++ci ) {\\n      #pragma unroll\\n      for( int ri = 0; ri < ROWS_PER_THREAD; ++ri ) {\\n\\n        // For Q/K, each warp loads from various timesteps. \\n        int ti = iter + warp*COLS_PER_THREAD;\\n        if( GO_BACKWARD ) {\\n          ti = params.L - 1 - ti;\\n        }\\n\\n        // Is it a valid access?\\n        int valid;\\n        if( GO_BACKWARD ) {\\n          valid = valid_qk[ri] && ti - ci >= 0;\\n        } else {\\n          valid = valid_qk[ri] && ti + ci < params.L;\\n        }\\n\\n        // The extra offset to add.\\n        if( GO_BACKWARD ) {\\n          offset_q = ri*THREADS_PER_WARP - ci*params.q_stride_L;\\n          offset_k = ri*THREADS_PER_WARP - ci*params.k_stride_L;\\n        } else {\\n          offset_q = ri*THREADS_PER_WARP + ci*params.q_stride_L;\\n          offset_k = ri*THREADS_PER_WARP + ci*params.k_stride_L;\\n        }\\n\\n        // Load Q/K if they are valid.\\n        q[ri][ci] = valid ? ptr_q[offset_q] : 0.f;\\n        k[ri][ci] = valid ? ptr_k[offset_k] : 0.f;\\n      }\\n    }\\n\\n    // For the V tensor, we assign contiguous thread to different loads. So, ti is different.\\n    int ti = iter + tidx;\\n    if( GO_BACKWARD ) {\\n      ti = params.L - 1 - ti;\\n    }\\n\\n    // Is it a valid access?\\n    int valid_vo = tidx < COLS_PER_ITER;\\n    if( GO_BACKWARD ) {\\n      valid_vo &= ti >= 0;\\n    } else {\\n      valid_vo &= ti < params.L;\\n    }\\n\\n    // Trigger the loads for V. \\n    float ldg_v = valid_vo ? *ptr_v : 0.f;\\n\\n    // Move the load pointers.\\n    if( GO_BACKWARD ) {\\n      ptr_q -= COLS_PER_ITER*params.q_stride_L;\\n      ptr_k -= COLS_PER_ITER*params.k_stride_L;\\n      ptr_v -= COLS_PER_ITER*params.v_stride_L;\\n    } else {\\n      ptr_q += COLS_PER_ITER*params.q_stride_L;\\n      ptr_k += COLS_PER_ITER*params.k_stride_L;\\n      ptr_v += COLS_PER_ITER*params.v_stride_L;\\n    }\\n\\n    // Store to shared memory.\\n    if( tidx < COLS_PER_ITER ) {\\n      smem_v[tidx] = ldg_v;\\n    }\\n\\n    // Make sure V is in shared memory.\\n    __syncthreads();\\n\\n    // Read V from shared memory.\\n    float v[COLS_PER_THREAD];\\n    #pragma unroll\\n    for( int ci = 0; ci < COLS_PER_THREAD; ++ci ) {\\n      v[ci] = smem_v[warp*COLS_PER_THREAD + ci];\\n    }\\n\\n    // Each thread computes local K*V products.\\n    float kv[ROWS_PER_THREAD][COLS_PER_THREAD];\\n    #pragma unroll\\n    for( int ri = 0; ri < ROWS_PER_THREAD; ++ri ) {\\n      #pragma unroll\\n      for( int ci = 0; ci < COLS_PER_THREAD; ++ci ) {\\n        kv[ri][ci] = 0.f;\\n      }\\n    }\\n\\n    // Update the K*V^T product.\\n    #pragma unroll\\n    for( int ci = 0; ci < COLS_PER_THREAD; ++ci ) {\\n      #pragma unroll\\n      for( int ri = 0; ri < ROWS_PER_THREAD; ++ri ) {\\n        kv[ri][ci] += k[ri][ci] * v[ci];\\n      }\\n    }\\n\\n    // We must perform the prefix sums within the thread-block. Start with the thread.\\n    #pragma unroll\\n    for( int ri = 0; ri < ROWS_PER_THREAD; ++ri ) {\\n      #pragma unroll\\n      for( int ci = 1; ci < COLS_PER_THREAD; ++ci ) {\\n        kv[ri][ci] += kv[ri][ci-1];\\n      }\\n    }\\n\\n    // Store the partial sums to shared memory. Unless we have no inter-warp reduction to perform.\\n    #pragma unroll\\n    for( int ri = 0; ri < ROWS_PER_THREAD; ++ri ) {\\n      smem_reds[warp*E + ri*THREADS_PER_WARP + lane] = kv[ri][COLS_PER_THREAD-1];\\n    }\\n\\n    // Make sure the data is in shared memory.\\n    __syncthreads();\\n\\n    // Each thread deals with one or more column(s) of the matrix.\\n    constexpr int SUMS_PER_THREAD = (E + THREADS_PER_BLOCK-1) / THREADS_PER_BLOCK;\\n    #pragma unroll\\n    for( int ii = 0, idx = tidx; ii < SUMS_PER_THREAD; ++ii, idx += THREADS_PER_BLOCK ) {\\n      if( idx < E ) {\\n        float sum = smem_reds[idx];\\n        #pragma unroll\\n        for( int jj = 1; jj < WARPS; ++jj ) {\\n          smem_reds[idx + jj*E] = sum += smem_reds[idx + jj*E];\\n        }\\n      }\\n    }\\n\\n    // Make sure the reductions are stored in shared memory.\\n    __syncthreads();\\n\\n    // Each thread updates his partial products.\\n    #pragma unroll\\n    for( int ri = 0; ri < ROWS_PER_THREAD; ++ri ) {\\n      float sum = running_kv[ri];\\n      if( warp > 0 ) {\\n        sum += smem_reds[(warp-1)*E + lane + ri*THREADS_PER_WARP];\\n      }\\n      #pragma unroll\\n      for( int ci = 0; ci < COLS_PER_THREAD; ++ci ) {\\n        kv[ri][ci] += sum;\\n      }\\n    }\\n\\n    // Compute the partial output values for that thread.\\n    float sum[COLS_PER_THREAD];\\n    #pragma unroll\\n    for( int ci = 0; ci < COLS_PER_THREAD; ++ci ) {\\n      sum[ci] = q[0][ci] * kv[0][ci];\\n      #pragma unroll\\n      for( int ri = 1; ri < ROWS_PER_THREAD; ++ri ) {\\n        sum[ci] += q[ri][ci] * kv[ri][ci];\\n      }\\n    }\\n\\n    // Run the parallel reductions inside the warp.\\n    #pragma unroll\\n    for( int mask = THREADS_PER_WARP / 2; mask >= 1; mask /= 2 ) {\\n      #pragma unroll\\n      for( int ci = 0; ci < COLS_PER_THREAD; ++ci ) {\\n        sum[ci] += __shfl_xor_sync(uint32_t(-1), sum[ci], mask);\\n      }\\n    }\\n\\n    // Store the final output to shared memory.\\n    if( lane == 0 ) {\\n      #pragma unroll\\n      for( int ci = 0; ci < COLS_PER_THREAD; ++ci ) {\\n        smem_o[warp*COLS_PER_THREAD + ci] = sum[ci];\\n      }\\n    }\\n\\n    // Make sure the data is in shared memory.\\n    __syncthreads();\\n\\n    // Store the output.\\n    if( valid_vo ) {\\n      *ptr_o = smem_o[tidx];\\n    }\\n\\n    // Each thread updates his running kv.\\n    #pragma unroll\\n    for( int ri = 0; ri < ROWS_PER_THREAD; ++ri ) {\\n      running_kv[ri] += smem_reds[(WARPS-1)*E + lane + ri*THREADS_PER_WARP];\\n    }\\n\\n    // Move to next location.\\n    if( GO_BACKWARD ) {\\n      ptr_o -= COLS_PER_ITER*params.o_stride_L;\\n    } else {\\n      ptr_o += COLS_PER_ITER*params.o_stride_L;\\n    }\\n  }\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\ntemplate< int E, bool GO_BACKWARD, int WARPS >\\nint lmha_low_occupancy_(const Lmha_params<float> &params) {\\n\\n  // Make sure we are not going to launch an invalid grid.\\n  if( params.H > 65535 || params.B > 65535 ) {\\n    return 1;\\n  }\\n\\n  // Prepare the grid and trigger the CUDA kernel.\\n  dim3 grid;\\n  grid.x = params.M;\\n  grid.y = params.H;\\n  grid.z = params.B;\\n  lmha_low_occupancy_kernel<E, GO_BACKWARD, WARPS><<<grid, WARPS*THREADS_PER_WARP>>>(params);\\n  return 0;\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\ntemplate< int E, bool GO_BACKWARD >\\nint lmha_low_occupancy_(const Lmha_params<float> &params, int blocks) {\\n         if( params.M * blocks >= 8*LOW_OCCUPANCY_THRESHOLD ) {\\n    return lmha_low_occupancy_<E, GO_BACKWARD,  4>(params);\\n  } else if( params.M * blocks >= 4*LOW_OCCUPANCY_THRESHOLD ) {\\n    return lmha_low_occupancy_<E, GO_BACKWARD,  8>(params);\\n  } else {\\n    return lmha_low_occupancy_<E, GO_BACKWARD, 16>(params);\\n  }\\n  return 1;\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\ntemplate< int E, typename Params >\\nstatic inline __device__ __host__ int smem_buffer_elts_(const Params &params) {\\n  int M = round_up(params.M, 4);\\n  return 2*E + 2*M;\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\ntemplate< int E, int THREADS_PER_HEAD, bool GO_BACKWARD >\\n__global__ \\nvoid lmha_kernel(Lmha_params<float> params) {\\n\\n  // Make sure E is a multiple of 4.\\n  static_assert(E % 4 == 0, \"\");\\n\\n  // The amount of shared memory per buffer (2 buffers for double-buffering).\\n  const int smem_buffer_elts = smem_buffer_elts_<E>(params);\\n  // The M dimension for shared memory.\\n  const int M = round_up(params.M, 4);\\n\\n  // Shared memory to store Q, K and V. Size is 2*smem_buffer_elts.\\n  extern __shared__ float smem_[];\\n\\n  // The various shared memory buffers.\\n  float *smem_q = &smem_[0*E];\\n  float *smem_k = &smem_[1*E];\\n  float *smem_v = &smem_[2*E];\\n  float *smem_o = &smem_[2*E + M];\\n\\n  // The index of the shared memory buffer (for double-buffering).\\n  int smem_curr = 0;\\n\\n  // The sequence processed by that block.\\n  const int bi = blockIdx.y;\\n  // The head processed by that block.\\n  const int hi = blockIdx.x;\\n\\n  // The linear index of the thread.\\n  const int tidx = threadIdx.x;\\n\\n  // The offset to the position loaded by the thread in Q.\\n  int offset_q = bi*params.q_stride_B + hi*params.q_stride_H + tidx;\\n  // The offset to the position loaded by the thread in K.\\n  int offset_k = bi*params.k_stride_B + hi*params.k_stride_H + tidx;\\n\\n  // If we walk backward, account for the extra offset.\\n  if( GO_BACKWARD ) {\\n    offset_q += (params.L-1)*params.q_stride_L;\\n    offset_k += (params.L-1)*params.k_stride_L;\\n  }\\n\\n  // Determine the base pointers for Q and K.\\n  const float *ptr_q = &params.q[offset_q];\\n  const float *ptr_k = &params.k[offset_k];\\n\\n  // The offset to the position loaded by the thread in V and O.\\n  int offset_v = bi*params.v_stride_B + hi*params.v_stride_H + tidx;\\n  int offset_o = bi*params.o_stride_B + hi*params.o_stride_H + tidx;\\n\\n  // If we walk backward, account for the extra offset.\\n  if( GO_BACKWARD ) {\\n    offset_v += (params.L-1)*params.v_stride_L;\\n    offset_o += (params.L-1)*params.o_stride_L;\\n  }\\n\\n  // Determine the base pointers for V.\\n  const float *ptr_v = &params.v[offset_v];\\n\\n  // Is it an active Q/K thread?\\n  const int active_qk = tidx < params.E;\\n\\n  // Trigger the memory loads for Q and K.\\n  float ldg_q = 0.f, ldg_k = 0.f;\\n  if( active_qk ) {\\n    ldg_q = *ptr_q;\\n    ldg_k = *ptr_k;\\n  }\\n\\n  // Is it an active V thread?\\n  const int active_v = tidx < params.M;\\n\\n  // Trigger the memory loads for V. \\n  float ldg_v = 0.f;\\n  if( active_v ) {\\n    ldg_v = *ptr_v;\\n  }\\n\\n  // Move the load pointers.\\n  if( GO_BACKWARD ) {\\n    ptr_q -= params.q_stride_L;\\n    ptr_k -= params.k_stride_L;\\n    ptr_v -= params.v_stride_L;\\n  } else {\\n    ptr_q += params.q_stride_L;\\n    ptr_k += params.k_stride_L;\\n    ptr_v += params.v_stride_L;\\n  }\\n\\n  // The number of FLOAT4s per head.\\n  constexpr int FLOAT4s_PER_HEAD = E / 4;\\n  // The number of FLOAT4s per thread.\\n  constexpr int FLOAT4s_PER_THREAD = FLOAT4s_PER_HEAD / THREADS_PER_HEAD;\\n\\n  // The storage for the K*V^T values.\\n  float4 kv[FLOAT4s_PER_THREAD]; \\n  #pragma unroll\\n  for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {\\n    kv[ii] = make_float4(0.f, 0.f, 0.f, 0.f);\\n  }\\n\\n  // The output pointer.\\n  float *out_ptr = &params.out[offset_o];\\n\\n  // Store to shared memory Q and K.\\n  if( tidx < E ) { \\n    smem_q[smem_curr*smem_buffer_elts + tidx] = ldg_q; \\n    smem_k[smem_curr*smem_buffer_elts + tidx] = ldg_k; \\n  }\\n\\n  // Store to shared memory V. All threads store valid values.\\n  if( tidx < M ) {\\n    smem_v[smem_curr*smem_buffer_elts + tidx] = ldg_v;\\n  }\\n\\n  // The position of the thread in the V dimension.\\n  int vo = tidx / THREADS_PER_HEAD;\\n  int vi = tidx % THREADS_PER_HEAD;\\n\\n  // Iterate over the timesteps.\\n  for( int ti = 0; ti < params.L; ++ti ) {\\n\\n    // Is it the last iteration?\\n    int is_last = ti == params.L - 1;\\n\\n    // Trigger the next loads for Q and K.\\n    if( !is_last && active_qk ) {\\n      ldg_q = *ptr_q;\\n      ldg_k = *ptr_k;\\n    }\\n\\n    // Trigger the next loads for V.\\n    if( !is_last && active_v ) {\\n      ldg_v = *ptr_v;\\n    }\\n\\n    // Move the load pointers.\\n    if( GO_BACKWARD ) {\\n      ptr_q -= params.q_stride_L;\\n      ptr_k -= params.k_stride_L;\\n      ptr_v -= params.v_stride_L;\\n    } else {\\n      ptr_q += params.q_stride_L;\\n      ptr_k += params.k_stride_L;\\n      ptr_v += params.v_stride_L;\\n    }\\n\\n    // Make sure the data is in shared memory.\\n    __syncthreads();\\n\\n    // Each thread loads 4 values from K.\\n    float4 k[FLOAT4s_PER_THREAD];\\n    #pragma unroll\\n    for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {\\n      int ki = tidx % THREADS_PER_HEAD * 4 + ii * THREADS_PER_HEAD * 4;\\n      k[ii] = *reinterpret_cast<const float4*>(&smem_k[smem_curr*smem_buffer_elts + ki]);\\n    }\\n\\n    // Each thread loads a single V value.\\n    float v = 0.f;\\n    if( vo < params.M ) {\\n      v = *reinterpret_cast<const float *>(&smem_v[smem_curr*smem_buffer_elts + vo]);\\n    }\\n\\n    // Update the K*V^T product.\\n    #pragma unroll\\n    for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {\\n      kv[ii].x += k[ii].x * v;\\n      kv[ii].y += k[ii].y * v;\\n      kv[ii].z += k[ii].z * v;\\n      kv[ii].w += k[ii].w * v;\\n    }\\n\\n    // Load the Q values from shared memory.\\n    float4 q[FLOAT4s_PER_THREAD]; \\n    #pragma unroll\\n    for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {\\n      int qi = tidx % THREADS_PER_HEAD * 4 + ii * THREADS_PER_HEAD * 4;\\n      q[ii] = *reinterpret_cast<const float4*>(&smem_q[smem_curr*smem_buffer_elts + qi]);\\n    }\\n\\n    // Compute the partial output value for that thread.\\n    float sum = 0.f;\\n    #pragma unroll\\n    for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {\\n      sum += q[ii].x * kv[ii].x;\\n      sum += q[ii].y * kv[ii].y;\\n      sum += q[ii].z * kv[ii].z;\\n      sum += q[ii].w * kv[ii].w;\\n    }\\n\\n    // Finalize the computation of the sum (if we have more than 1 thread per head).\\n    if( THREADS_PER_HEAD > 1 ) {\\n\\n      // Finalize the sum for each head.\\n      #pragma unroll\\n      for( int mask = THREADS_PER_HEAD / 2; mask >= 1; mask /= 2 ) {\\n        sum += __shfl_xor_sync(uint32_t(-1), sum, mask);\\n      }\\n\\n      // Store to shared memory.\\n      if( vo < M && vi == 0 ) {\\n        smem_o[smem_curr*smem_buffer_elts + vo] = sum;\\n      }\\n\\n      // Make sure the data is in shared memory.\\n      __syncthreads();\\n\\n      // Active threads read the data to store.\\n      if( active_v ) {\\n        sum = smem_o[smem_curr*smem_buffer_elts + tidx];\\n      }\\n\\n    } // THREADS_PER_HEAD > 1.\\n\\n    // Store the output. All the threads are active.\\n    if( active_v ) {\\n      *out_ptr = sum;\\n    }\\n\\n    // Move to next location.\\n    if( GO_BACKWARD ) {\\n      out_ptr -= params.o_stride_L;\\n    } else {\\n      out_ptr += params.o_stride_L;\\n    }\\n\\n    // Move the shared memory buffer.\\n    smem_curr = (smem_curr + 1) % 2;\\n\\n    // Store to shared memory for Q and K.\\n    if( !is_last && tidx < E ) {\\n      smem_q[smem_curr*smem_buffer_elts + tidx] = ldg_q;\\n      smem_k[smem_curr*smem_buffer_elts + tidx] = ldg_k;\\n    }\\n\\n    // Store to shared memory for V.\\n    if( !is_last && tidx < M ) {\\n      smem_v[smem_curr*smem_buffer_elts + tidx] = ldg_v;\\n    }\\n  }\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\ntemplate< int E, int THREADS_PER_HEAD, bool GO_BACKWARD >\\nint lmha_(const Lmha_params<float> &params) {\\n  // The M dimension rounded up to 4.\\n  int M = round_up(params.M, 4);\\n\\n  // The number of threads in the block.\\n  int block = round_up(max(E, M*THREADS_PER_HEAD), 32);\\n  if( block > 512 || params.B > 65535 ) {\\n    return 1;\\n  }\\n\\n  // Prepare the kernel.\\n  dim3 grid(params.H, params.B);\\n  size_t smem = smem_buffer_elts_<E>(params)*2*sizeof(float);\\n  lmha_kernel<E, THREADS_PER_HEAD, GO_BACKWARD><<<grid, block, smem>>>(params);\\n  return 0;\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\ntemplate< bool GO_BACKWARD >\\nint lmha(const Lmha_params<float> &params) {\\n  int blocks = params.B * params.H;\\n  int res = 1;\\n  if( blocks < LOW_OCCUPANCY_THRESHOLD ) { \\n           if( params.E <=  32 ) {\\n      res = lmha_low_occupancy_< 32, GO_BACKWARD>(params, blocks);\\n    } else if( params.E <=  64 ) {\\n      res = lmha_low_occupancy_< 64, GO_BACKWARD>(params, blocks);\\n    } else if( params.E <= 128 ) {\\n      res = lmha_low_occupancy_<128, GO_BACKWARD>(params, blocks);\\n    } else if( params.E <= 256 ) {\\n      res = lmha_low_occupancy_<256, GO_BACKWARD>(params, blocks);\\n    }\\n  } else {\\n           if( params.E <=  32 ) {\\n      res = lmha_< 32, 1, GO_BACKWARD>(params);\\n    } else if( params.E <=  48 ) {\\n      res = lmha_< 48, 1, GO_BACKWARD>(params);\\n    } else if( params.E <=  64 ) {\\n      res = lmha_< 64, 1, GO_BACKWARD>(params);\\n    } else if( params.E <= 128 ) {\\n      res = lmha_<128, 2, GO_BACKWARD>(params);\\n    } else if( params.E <= 256 ) {\\n      res = lmha_<256, 4, GO_BACKWARD>(params);\\n    }\\n  }\\n  return res;\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\ntemplate< typename T >\\ninline void set_params(Lmha_params<T> &params,\\n                       const torch::Tensor q,\\n                       const torch::Tensor k,\\n                       const torch::Tensor v,\\n                       torch::Tensor       o) {\\n\\n  // Define the pointers.\\n  params.out = o.data_ptr<T>();\\n  params.q   = q.data_ptr<T>();\\n  params.k   = k.data_ptr<T>();\\n  params.v   = v.data_ptr<T>();\\n\\n  // Define the strides.\\n  params.q_stride_B = (int) q.stride(0);\\n  params.q_stride_H = (int) q.stride(1);\\n  params.q_stride_L = (int) q.stride(2);\\n  params.k_stride_B = (int) k.stride(0);\\n  params.k_stride_H = (int) k.stride(1);\\n  params.k_stride_L = (int) k.stride(2);\\n  params.v_stride_B = (int) v.stride(0);\\n  params.v_stride_H = (int) v.stride(1);\\n  params.v_stride_L = (int) v.stride(2);\\n  params.o_stride_B = (int) o.stride(0);\\n  params.o_stride_H = (int) o.stride(1);\\n  params.o_stride_L = (int) o.stride(2);\\n\\n  // Extract the dimensions.\\n  int N = q.size(0);\\n  int H = q.size(1);\\n  int L = q.size(2);\\n  int E = q.size(3);\\n  int M = v.size(3);\\n\\n  params.B = N;\\n  params.L = L;\\n  params.H  = H;\\n  params.E = E;\\n  params.M = M;\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\nint lmha_fwd(const torch::Tensor queries,\\n             const torch::Tensor keys,\\n             const torch::Tensor values,\\n             torch::Tensor product) {\\n\\n  // Make sure that we are using the correct GPU device\\n  torch::DeviceGuard _guard(queries.device());\\n\\n  // Make sure the inner-most dimension of the tensors is packed.\\n  assert(queries.stride(3) == 1);\\n  assert(keys   .stride(3) == 1);\\n  assert(values .stride(3) == 1);\\n  assert(product.stride(3) == 1);\\n\\n  // Extract the dimensions.\\n  int N = queries.size(0);\\n  int H = queries.size(1);\\n  int L = queries.size(2);\\n  int E = queries.size(3);\\n  int M = values.size (3);\\n\\n  // The structure of params.\\n  Lmha_params<float> params;\\n  set_params(params, queries, keys, values, product);\\n\\n  // Launch the kernel.\\n  return lmha<false>(params);\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\ntemplate< typename T >\\nstruct Lmha_bwd_params {\\n\\n  // The output buffer for K. Dimensions [B, H, L, D].\\n  T *out_k;\\n  // The output buffer for V. Dimensions [B, H, L, D].\\n  T *out_v;\\n\\n  // The input Qs. Dimensions [B, H, L, D].\\n  const T *q;\\n  // The input Ks. Dimensions [B, H, L, D].\\n  const T *k;\\n  // The input Vs. Dimensions [B, H, L, D].\\n  const T *v;\\n  // The input Gs. Dimensions [B, H, L, D].\\n  const T *g;\\n\\n  // The dimensions.\\n  int B, L, H, M, E;\\n\\n  // The strides for the input tensors.\\n  int q_stride_B, q_stride_L, q_stride_H;\\n  int k_stride_B, k_stride_L, k_stride_H;\\n  int v_stride_B, v_stride_L, v_stride_H;\\n  int g_stride_B, g_stride_L, g_stride_H;\\n\\n  // The strides for the outputs.\\n  int out_k_stride_B, out_k_stride_L, out_k_stride_H;\\n  int out_v_stride_B, out_v_stride_L, out_v_stride_H;\\n};\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\ntemplate< int D, int THREADS_PER_HEAD >\\n__global__ __launch_bounds__(D*THREADS_PER_HEAD*2)\\nvoid lmha_bwd_kernel(Lmha_bwd_params<float> params) {\\n\\n  // Make sure D is a multiple of 4.\\n  static_assert(D % 4 == 0, \"\");\\n\\n  // The shared memory buffers.\\n  __shared__ struct Smem { float qg[2*D], kv[2*D], out_kv[2*D]; } smem_[2];\\n\\n  // The index of the shared memory buffer (for double-buffering).\\n  int smem_curr = 0;\\n\\n  // The sequence processed by that block.\\n  const int bi = blockIdx.y;\\n  // The head processed by that block.\\n  const int hi = blockIdx.x;\\n\\n  // The linear index of the thread.\\n  const int tidx = threadIdx.x;\\n\\n  // Split the threads into two slices.\\n  int so = tidx / (D*THREADS_PER_HEAD);\\n  int si = tidx % (D*THREADS_PER_HEAD);\\n\\n  // The strides for B/L/H for the Q/G tensors.\\n  int qg_stride_B, qg_stride_L, qg_stride_H;\\n  if( so == 0 ) {\\n    qg_stride_B = params.q_stride_B;\\n    qg_stride_L = params.q_stride_L;\\n    qg_stride_H = params.q_stride_H;\\n  } else {\\n    qg_stride_B = params.g_stride_B;\\n    qg_stride_L = params.g_stride_L;\\n    qg_stride_H = params.g_stride_H;\\n  }\\n\\n  // The strides for B/L/H for the K/V tensors.\\n  int kv_stride_B, kv_stride_L, kv_stride_H;\\n  if( so == 0 ) {\\n    kv_stride_B = params.k_stride_B;\\n    kv_stride_L = params.k_stride_L;\\n    kv_stride_H = params.k_stride_H;\\n  } else {\\n    kv_stride_B = params.v_stride_B;\\n    kv_stride_L = params.v_stride_L;\\n    kv_stride_H = params.v_stride_H;\\n  }\\n\\n  // The hidden size.\\n  int hidden_size_per_head = 0;\\n  if( so == 0 ) {\\n    hidden_size_per_head = params.E;\\n  } else {\\n    hidden_size_per_head = params.M;\\n  }\\n\\n  // Where to start reading from.\\n  int offset_qg = bi*qg_stride_B + hi*qg_stride_H + si;\\n  int offset_kv = bi*kv_stride_B + hi*kv_stride_H + si;\\n\\n  // We walk backward, account for the extra offset.\\n  offset_qg += (params.L-1)*qg_stride_L;\\n  offset_kv += (params.L-1)*kv_stride_L;\\n\\n  // Determine the base pointers for Q, K, V and G.\\n  const float *ptr_qg = &(so == 0 ? params.q : params.g)[offset_qg];\\n  const float *ptr_kv = &(so == 0 ? params.k : params.v)[offset_kv]; \\n\\n  // Is it an active thread?\\n  const int active = si < hidden_size_per_head;\\n\\n  // Trigger the memory loads for Q, K, V and G.\\n  float ldg_qg = 0.f, ldg_kv = 0.f;\\n  if( active ) {\\n    ldg_qg = *ptr_qg;\\n    ldg_kv = *ptr_kv;\\n  }\\n\\n  // Move the load pointers (backward).\\n  ptr_qg -= qg_stride_L;\\n  ptr_kv -= kv_stride_L;\\n\\n  // The number of FLOAT4s per head.\\n  constexpr int FLOAT4s_PER_HEAD = D / 4;\\n  // The number of FLOAT4s per thread.\\n  constexpr int FLOAT4s_PER_THREAD = FLOAT4s_PER_HEAD / THREADS_PER_HEAD;\\n\\n  // The storage for the G*Q^T or Q^T*G values.\\n  float4 gq[FLOAT4s_PER_THREAD]; \\n  #pragma unroll\\n  for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {\\n    gq[ii] = make_float4(0.f, 0.f, 0.f, 0.f);\\n  }\\n\\n  // The strides for B/L/H for the K/V tensors.\\n  int out_kv_stride_B, out_kv_stride_L, out_kv_stride_H;\\n  if( so == 0 ) {\\n    out_kv_stride_B = params.out_k_stride_B;\\n    out_kv_stride_L = params.out_k_stride_L;\\n    out_kv_stride_H = params.out_k_stride_H;\\n  } else {\\n    out_kv_stride_B = params.out_v_stride_B;\\n    out_kv_stride_L = params.out_v_stride_L;\\n    out_kv_stride_H = params.out_v_stride_H;\\n  }\\n\\n  // Where to start reading from.\\n  int offset_out_kv = bi*out_kv_stride_B + hi*out_kv_stride_H + si;\\n\\n  // We walk backward, account for the extra offset.\\n  offset_out_kv += (params.L-1)*out_kv_stride_L;\\n\\n  // The output pointer.\\n  float *ptr_out_kv = &(so == 0 ? params.out_k : params.out_v)[offset_out_kv];\\n\\n  // Store to shared memory.\\n  if( si < D ) { \\n    smem_[smem_curr].qg[so*D + si] = ldg_qg; \\n    smem_[smem_curr].kv[so*D + si] = ldg_kv; \\n  }\\n\\n  // The position of the thread in the output dimension.\\n  int oo = si / THREADS_PER_HEAD % D;\\n  int oi = si % THREADS_PER_HEAD * 4;\\n\\n  // Iterate over the timesteps.\\n  for( int ti = 0; ti < params.L; ++ti ) {\\n\\n    // Is it the last iteration?\\n    int is_last = ti == params.L - 1;\\n\\n    // Trigger the next loads.\\n    if( !is_last && active ) {\\n      ldg_qg = *ptr_qg;\\n      ldg_kv = *ptr_kv;\\n    }\\n\\n    // Move the load pointers.\\n    ptr_qg -= qg_stride_L;\\n    ptr_kv -= kv_stride_L;\\n\\n    // Make sure the data is in shared memory.\\n    __syncthreads();\\n\\n    // Each thread loads 4 values from G or Q.\\n    float4 g[FLOAT4s_PER_THREAD];\\n    #pragma unroll\\n    for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {\\n      float *smem_ptr = &smem_[smem_curr].qg[(so^1)*D + oi];\\n      g[ii] = *reinterpret_cast<const float4*>(&smem_ptr[ii*THREADS_PER_HEAD*4]);\\n    }\\n\\n    // Each thread loads a single from Q or G value.\\n    float q = smem_[smem_curr].qg[so*D + oo];\\n\\n    // Update the G*Q^T or Q*G^T product.\\n    #pragma unroll\\n    for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {\\n      gq[ii].x += g[ii].x * q;\\n      gq[ii].y += g[ii].y * q;\\n      gq[ii].z += g[ii].z * q;\\n      gq[ii].w += g[ii].w * q;\\n    }\\n\\n    // Load the V or K values from shared memory.\\n    float4 v[FLOAT4s_PER_THREAD]; \\n    #pragma unroll\\n    for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {\\n      float *smem_ptr = &smem_[smem_curr].kv[(so^1)*D + oi];\\n      v[ii] = *reinterpret_cast<const float4*>(&smem_ptr[ii*THREADS_PER_HEAD*4]);\\n    }\\n\\n    // Compute the partial output value for that thread.\\n    float sum = 0.f;\\n    #pragma unroll\\n    for( int ii = 0; ii < FLOAT4s_PER_THREAD; ++ii ) {\\n      sum += v[ii].x * gq[ii].x;\\n      sum += v[ii].y * gq[ii].y;\\n      sum += v[ii].z * gq[ii].z;\\n      sum += v[ii].w * gq[ii].w;\\n    }\\n\\n    // Finalize the computation of the sum (if we have more than 1 thread per head).\\n    if( THREADS_PER_HEAD > 1 ) {\\n\\n      // Finalize the sum for each head.\\n      #pragma unroll\\n      for( int mask = THREADS_PER_HEAD / 2; mask >= 1; mask /= 2 ) {\\n        sum += __shfl_xor_sync(uint32_t(-1), sum, mask);\\n      }\\n\\n      // Store to shared memory.\\n      if( oi == 0 ) {\\n        smem_[smem_curr].out_kv[so*D + oo] = sum;\\n      }\\n\\n      // Make sure the data is in shared memory.\\n      __syncthreads();\\n\\n      // Active threads read the data to store.\\n      if( si < hidden_size_per_head ) {\\n        sum = smem_[smem_curr].out_kv[so*D + si];\\n      }\\n\\n    } // THREADS_PER_HEAD > 1.\\n\\n    // Store the output. All the threads are active.\\n    if( si < hidden_size_per_head ) {\\n      *ptr_out_kv = sum;\\n    }\\n\\n    // Move to next location.\\n    ptr_out_kv -= out_kv_stride_L;\\n\\n    // Move the shared memory buffer.\\n    smem_curr = (smem_curr + 1) % 2;\\n\\n    // Store to shared memory for Q and K.\\n    if( !is_last && si < D ) {\\n      smem_[smem_curr].qg[so*D + si] = ldg_qg; \\n      smem_[smem_curr].kv[so*D + si] = ldg_kv; \\n    }\\n  }\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\ntemplate< int D, int THREADS_PER_HEAD >\\nint lmha_bwd_(const Lmha_bwd_params<float> &params) {\\n  int block = D*THREADS_PER_HEAD*2;\\n  if( block >= 1024 || params.B > 65535 ) {\\n    return 1;\\n  }\\n  dim3 grid(params.H, params.B);\\n  lmha_bwd_kernel<D, THREADS_PER_HEAD><<<grid, block>>>(params);\\n  return 0;\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\nint lmha_bwd(const Lmha_bwd_params<float> &params) {\\n  int blocks = params.B * params.H;\\n  if( blocks < LOW_OCCUPANCY_THRESHOLD ) { \\n    return 1;\\n  }\\n\\n  int hidden_size_per_head = max(params.E, params.M);\\n  int res = 1;\\n  if( hidden_size_per_head <= 32 ) {\\n    res = lmha_bwd_< 32, 1>(params);\\n  } else if( hidden_size_per_head <= 64 ) {\\n    res = lmha_bwd_< 64, 1>(params);\\n  } else if( hidden_size_per_head <= 128 ) {\\n    res = lmha_bwd_<128, 2>(params);\\n  } else if( hidden_size_per_head <= 256 ) {\\n    res = lmha_bwd_<256, 4>(params);\\n  }\\n  return res;\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\nint lmha_bwd(const torch::Tensor queries,\\n             const torch::Tensor keys,\\n             const torch::Tensor values,\\n             const torch::Tensor grad_out,\\n             torch::Tensor grad_queries,\\n             torch::Tensor grad_keys,\\n             torch::Tensor grad_values) {\\n\\n  // Make sure that we are using the correct GPU device\\n  torch::DeviceGuard _guard(queries.device());\\n\\n  // Make sure the inner-most dimension of the tensors is packed.\\n  assert(queries     .stride(3) == 1);\\n  assert(keys        .stride(3) == 1);\\n  assert(values      .stride(3) == 1);\\n  assert(grad_out    .stride(3) == 1);\\n  assert(grad_queries.stride(3) == 1);\\n  assert(grad_keys   .stride(3) == 1);\\n  assert(grad_values .stride(3) == 1);\\n\\n  // Extract the dimensions.\\n  int N = queries.size(0);\\n  int H = queries.size(1);\\n  int L = queries.size(2);\\n  int E = queries.size(3);\\n  int M = values.size (3);\\n\\n  // Gradient on Q.\\n\\n  // The structure of params.\\n  Lmha_params<float> params;\\n  set_params(params, grad_out, values, keys, grad_queries);\\n\\n  // Launch the kernel.\\n  int res = lmha<false>(params);\\n  if( res ) {\\n    return res;\\n  }\\n\\n  // Gradient on K and V together.\\n\\n  Lmha_bwd_params<float> bwd_params;\\n  bwd_params.out_k = grad_keys.data_ptr<float>();\\n  bwd_params.out_v = grad_values.data_ptr<float>();\\n  bwd_params.q = queries.data_ptr<float>();\\n  bwd_params.k = keys.data_ptr<float>();\\n  bwd_params.v = values.data_ptr<float>();\\n  bwd_params.g = grad_out.data_ptr<float>();\\n\\n  bwd_params.B = N;\\n  bwd_params.L = L;\\n  bwd_params.H = H;\\n  bwd_params.E = E;\\n  bwd_params.M = M;\\n\\n  bwd_params.q_stride_B = queries.stride(0);\\n  bwd_params.q_stride_H = queries.stride(1);\\n  bwd_params.q_stride_L = queries.stride(2);\\n  bwd_params.k_stride_B = keys.stride(0);\\n  bwd_params.k_stride_H = keys.stride(1);\\n  bwd_params.k_stride_L = keys.stride(2);\\n  bwd_params.v_stride_B = values.stride(0);\\n  bwd_params.v_stride_H = values.stride(1);\\n  bwd_params.v_stride_L = values.stride(2);\\n  bwd_params.g_stride_B = grad_out.stride(0);\\n  bwd_params.g_stride_H = grad_out.stride(1);\\n  bwd_params.g_stride_L = grad_out.stride(2);\\n\\n  bwd_params.out_k_stride_B = grad_keys.stride(0);\\n  bwd_params.out_k_stride_H = grad_keys.stride(1);\\n  bwd_params.out_k_stride_L = grad_keys.stride(2);\\n  bwd_params.out_v_stride_B = grad_values.stride(0);\\n  bwd_params.out_v_stride_H = grad_values.stride(1);\\n  bwd_params.out_v_stride_L = grad_values.stride(2);\\n\\n  // Try to run the fused kernel.\\n  int fallback = lmha_bwd(bwd_params);\\n\\n  // If it failed, fallback on separate kernels for K and V.\\n  if( fallback ) {\\n\\n    // Gradient on K.\\n\\n    // Launch the kernel.\\n    set_params(params, values, grad_out, queries, grad_keys);\\n    res = lmha<true>(params);\\n    if( res ) {\\n      return res;\\n    }\\n\\n    // Gradient on V.\\n\\n    // Launch the kernel.\\n    set_params(params, keys, queries, grad_out, grad_values);\\n    return lmha<true>(params);\\n  }\\n\\n  // It worked...\\n  return 0;\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\n} // namespace nvidia\\n#endif // #ifdef ENABLE_NVIDIA_OPTIMIZATIONS\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\ntypedef torch::PackedTensorAccessor32<float, 4, torch::RestrictPtrTraits> float_accessor;\\n\\n#define E_BLOCK_SIZE 8\\n\\n__global__ void causal_dot_product_kernel(\\n    const float_accessor queries,\\n    const float_accessor keys,\\n    const float_accessor values,\\n    float_accessor result,\\n    const int N,\\n    const int H,\\n    const int L,\\n    const int E,\\n    const int M\\n) {\\n    int n = blockIdx.y;\\n    int h = blockIdx.z;\\n\\n    int e_start = blockIdx.x * E_BLOCK_SIZE;\\n    int m = threadIdx.x % M;\\n\\n    extern __shared__ float shared_mem[];\\n    float* shared_kv = shared_mem;\\n\\n    for (int e_local = 0; e_local < E_BLOCK_SIZE && e_local + e_start < E; e_local++) {\\n      shared_kv[m + e_local * M] = 0;\\n    }\\n\\n    for (int t=0; t<L; t++) {\\n      float res = 0;\\n      for (int e_local = 0; e_local < E_BLOCK_SIZE && e_local + e_start < E; e_local++) {\\n        shared_kv[e_local*M + m] += keys[n][h][t][e_local + e_start] * values[n][h][t][m];\\n        res += queries[n][h][t][e_local + e_start] * shared_kv[e_local*M + m];\\n      }\\n      atomicAdd(\\n          &result[n][h][t][m],\\n          res\\n      );\\n    }\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\nvoid causal_dot_product_(const torch::Tensor queries,\\n                         const torch::Tensor keys,\\n                         const torch::Tensor values,\\n                         torch::Tensor product) {\\n    // Make sure that we are using the correct GPU device\\n    torch::DeviceGuard _guard(queries.device());\\n\\n    int N = queries.size(0);\\n    int H = queries.size(1);\\n    int L = queries.size(2);\\n    int E = queries.size(3);\\n    int M = values.size(3);\\n\\n    const int blocks_per_sequence = (E + E_BLOCK_SIZE - 1) / E_BLOCK_SIZE;\\n\\n    dim3 blockDim(M, 1, 1);\\n    dim3 gridDim(blocks_per_sequence, N, H);\\n    const int shared_mem_forward = E_BLOCK_SIZE * M * sizeof(float);\\n\\n    causal_dot_product_kernel<<<gridDim, blockDim, shared_mem_forward>>>(\\n      queries.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),\\n      keys.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),\\n      values.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),\\n      product.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),\\n      N, H, L, E, M\\n    );\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\nvoid causal_dot_product(const torch::Tensor queries,\\n                        const torch::Tensor keys,\\n                        const torch::Tensor values,\\n                        torch::Tensor product) {\\n#ifdef ENABLE_NVIDIA_OPTIMIZATIONS\\n  int fallback = nvidia::lmha_fwd(queries, keys, values, product);\\n#else\\n  int fallback = 1;\\n#endif\\n  if( fallback ) {\\n    causal_dot_product_(queries, keys, values, product);\\n  }\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\n#define M_BLOCK_SIZE 4\\n\\n// we need shared memory to store\\n// kv\\n// Backward direction\\n// kv_backwards\\n// Shared memory usage\\n__global__ void causal_dot_backward_query_key_kernel(\\n    const float_accessor queries,\\n    const float_accessor keys,\\n    const float_accessor values,\\n    const float_accessor grad_out,\\n    float_accessor grad_queries,\\n    float_accessor grad_keys,\\n    int N,\\n    int H,\\n    int L,\\n    int E,\\n    int M\\n) {\\n    int n = blockIdx.y;\\n    int h = blockIdx.z;\\n\\n    int m_start = blockIdx.x * M_BLOCK_SIZE;\\n    int e = threadIdx.x % E;\\n\\n    extern __shared__ float shared_mem[];\\n    const int shared_kv_size = M_BLOCK_SIZE * E;\\n    float* shared_kv = shared_mem;\\n    float* shared_kv_bw = shared_mem + shared_kv_size;\\n\\n    for (int m_local = 0; m_local < M_BLOCK_SIZE && m_local + m_start < M; m_local++) {\\n      shared_kv[m_local * E + e] = 0;\\n      shared_kv_bw[m_local * E + e] = 0;\\n    }\\n\\n    for (int l=0; l<L; l++) {\\n      float res = 0, res_bw = 0;\\n      int l_b = L - l - 1;\\n      for (int m_local = 0; m_local < M_BLOCK_SIZE && m_local + m_start < M; m_local++) {\\n        shared_kv[m_local*E + e] += keys[n][h][l][e] * values[n][h][l][m_start + m_local];\\n        shared_kv_bw[m_local*E + e] += queries[n][h][l_b][e] * grad_out[n][h][l_b][m_start + m_local];\\n        res += grad_out[n][h][l][m_start + m_local] * shared_kv[m_local*E + e];\\n        res_bw += values[n][h][l_b][m_start + m_local] * shared_kv_bw[m_local*E + e];\\n      }\\n      atomicAdd(\\n        &grad_queries[n][h][l][e],\\n        res\\n      );\\n      atomicAdd(\\n        &grad_keys[n][h][l_b][e],\\n        res_bw\\n      );\\n    }\\n}\\n\\n\\n__global__ void causal_dot_backward_value_kernel(\\n    const float_accessor queries,\\n    const float_accessor keys,\\n    const float_accessor values,\\n    const float_accessor grad_out,\\n    float_accessor grad_keys,\\n    float_accessor grad_values,\\n    int N,\\n    int H,\\n    int L,\\n    int E,\\n    int M\\n) {\\n    int n = blockIdx.y;\\n    int h = blockIdx.z;\\n\\n    int e_start = blockIdx.x * E_BLOCK_SIZE;\\n    int m = threadIdx.x % M;\\n\\n    extern __shared__ float shared_mem[];\\n    float* shared_kv = shared_mem;\\n    for (int e_local = 0; e_local < E_BLOCK_SIZE && e_local + e_start < E; e_local++) {\\n      shared_kv[m + e_local * M] = 0;\\n    }\\n\\n    for (int l = 0; l < L; l++) {\\n        int l_b = L - l -1;\\n        float res = 0;\\n        for (int e_local = 0; e_local < E_BLOCK_SIZE && e_local + e_start < E; e_local++) {\\n          shared_kv[e_local*M + m] += queries[n][h][l_b][e_start + e_local] * grad_out[n][h][l_b][m];\\n          res += keys[n][h][l_b][e_start + e_local] * shared_kv[e_local*M + m];\\n        }\\n        atomicAdd(\\n            &grad_values[n][h][l_b][m],\\n            res\\n        );\\n    }\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\nvoid causal_dot_backward_(const torch::Tensor queries,\\n                          const torch::Tensor keys,\\n                          const torch::Tensor values,\\n                          const torch::Tensor grad_out,\\n                          torch::Tensor grad_queries,\\n                          torch::Tensor grad_keys,\\n                          torch::Tensor grad_values) {\\n\\n    // Make sure that we are using the correct GPU device\\n    torch::DeviceGuard _guard(queries.device());\\n\\n    int N = queries.size(0);\\n    int H = queries.size(1);\\n    int L = queries.size(2);\\n    int E = queries.size(3);\\n    int M = values.size(3);\\n\\n    const int blocks_per_sequence = (M + M_BLOCK_SIZE - 1) / M_BLOCK_SIZE;\\n\\n    dim3 blockDim(E, 1, 1);\\n    dim3 gridDim(blocks_per_sequence, N, H);\\n    const int shared_mem_qk_backward = 2 * M_BLOCK_SIZE * E * sizeof(float);\\n\\n    causal_dot_backward_query_key_kernel<<<gridDim, blockDim, shared_mem_qk_backward>>>(\\n      queries.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),\\n      keys.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),\\n      values.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),\\n      grad_out.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),\\n      grad_queries.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),\\n      grad_keys.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),\\n      N, H, L, E, M\\n    );\\n\\n    const int blocks_per_sequence_value = (E + E_BLOCK_SIZE - 1) / E_BLOCK_SIZE;\\n\\n    dim3 blockDimv(M, 1, 1);\\n    dim3 gridDimv(blocks_per_sequence_value, N, H);\\n    const int shared_mem_v_backward = E_BLOCK_SIZE * M * sizeof(float);\\n    causal_dot_backward_value_kernel<<<gridDimv, blockDimv, shared_mem_v_backward>>>(\\n      queries.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),\\n      keys.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),\\n      values.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),\\n      grad_out.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),\\n      grad_keys.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),\\n      grad_values.packed_accessor32<float, 4, torch::RestrictPtrTraits>(),\\n      N, H, L, E, M\\n    );\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\nvoid causal_dot_backward(const torch::Tensor queries,\\n                         const torch::Tensor keys,\\n                         const torch::Tensor values,\\n                         const torch::Tensor grad_out,\\n                         torch::Tensor grad_queries,\\n                         torch::Tensor grad_keys,\\n                         torch::Tensor grad_values) {\\n#ifdef ENABLE_NVIDIA_OPTIMIZATIONS\\n  int fallback = nvidia::lmha_bwd(queries,\\n                                  keys,\\n                                  values,\\n                                  grad_out,\\n                                  grad_queries,\\n                                  grad_keys,\\n                                  grad_values);\\n#else\\n  int fallback = 1;\\n#endif\\n  if( fallback ) {\\n    // Make sure that the gradient tensors are 0. This is needed because the\\n    // bwd pass might have partially executed and filled in some values in\\n    // grad_queries or grad_keys.\\n    //\\n    // This adds a small overhead every time we have to fall back to the old\\n    // kernel for the backward pass.\\n    grad_queries.zero_();\\n    grad_keys.zero_();\\n    causal_dot_backward_(queries, keys, values, grad_out, grad_queries, grad_keys, grad_values);\\n  }\\n}\\n\\n////////////////////////////////////////////////////////////////////////////////////////////////////\\n\\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\\n    m.def(\\n        \"causal_dot_product\",\\n        &causal_dot_product,\\n        \"Compute the weighted sum of values but attending only to previous \"\\n        \"values.\"\\n    );\\n    m.def(\\n        \"causal_dot_backward\",\\n        &causal_dot_backward,\\n        \"Compute the gradients for the causal dot product.\"\\n    );\\n}\\n\\n', additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content='What does this file look like to you?', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='This file, `causal_product_cuda.cu`, looks like a CUDA/C++ source file implementing high-performance custom CUDA kernels for **causal dot-product attention**, typically used in deep learning—particularly in transformer models where causal (autoregressive) attention is required (e.g., for language modeling). The main features and observations are:\\n\\n---\\n\\n### 1. **CUDA Kernels for Attention**\\n\\n- The file contains multiple CUDA kernel implementations (e.g., `causal_dot_product_kernel`, `causal_dot_backward_query_key_kernel`, etc.) designed to run on GPUs for efficient computation of forward and backward passes of causal attention.\\n- Shared memory is used for performance optimization.\\n- There’s heavy loop unrolling, atomic operations, and other standard CUDA optimizations.\\n\\n---\\n\\n### 2. **NVIDIA Optimizations**\\n\\n- There is a large code block within the `nvidia` namespace, enabled using `#define ENABLE_NVIDIA_OPTIMIZATIONS`.\\n- These sections offer highly optimized paths for specific tensor layouts and CUDA hardware, leveraging warp and shared memory structures.\\n- There are templated functions and device/host code (`__device__ __host__`).\\n\\n---\\n\\n### 3. **Integration with PyTorch**\\n\\n- At the end, there is a `PYBIND11_MODULE` block, exposing some functions (like `causal_dot_product` and `causal_dot_backward`) to Python as a PyTorch extension.\\n- All tensor inputs and outputs use the PyTorch C++ API (`torch::Tensor`, `PackedTensorAccessor32`, etc.).\\n- It uses the C++ extension API to integrate custom CUDA ops with PyTorch.\\n\\n---\\n\\n### 4. **Purpose and Use-Case**\\n\\n- The file is likely a core part of a custom attention mechanism for transformer-type models, focused on achieving high-speed (possibly low-memory) implementations of the causal dot-product operation and its gradient/backward pass.\\n- Used in contexts where standard PyTorch attention is not fast enough, or when large-sequence models (like large language models) require specialized kernel optimizations.\\n\\n---\\n\\n### 5. **Copyright and Authorship**\\n\\n- It credits researchers from the Idiap Research Institute and NVIDIA.\\n- The copyright/license blocks indicate:\\n  - Originated from academic research.\\n  - Parts are contributed/modified by NVIDIA.\\n\\n---\\n\\n### 6. **Fallback Mechanisms**\\n\\n- The code checks if the NVIDIA-optimized path is available; otherwise, it falls back to a native implementation.\\n\\n---\\n\\n**Summary:**  \\nThis file is a **custom CUDA implementation for causal dot-product attention** and its gradients, highly optimized (with special NVIDIA kernels), meant as a PyTorch extension for deep learning models, especially those needing fast autoregressive/causal attention for long sequences.\\n\\nIf you’d like an explanation of the logic of any specific kernel, an architectural overview, or details on how to build/use this as a PyTorch extension, let me know!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "import app_memory_hook\n",
    "import pprint\n",
    "pprint.pprint(app_memory_hook.chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chainlit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
